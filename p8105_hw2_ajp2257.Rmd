---
title: "p8105 HW2"
author: "Amy Pitts"
date: "9/30/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(arsenal)
```

# Problem 1
##### This problem uses the Mr. Trash Wheel dataset, available as an Excel file on the course website.

##### Read and clean the Mr. Trash Wheel sheet:
- specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel
- use reasonable variable names
- omit rows that do not include dumpster-specific data
- round the number of sports balls to the nearest integer and converts the result to an integer variable (using as.integer)

##### Read and clean precipitation data for 2017 and 2018. For each, omit rows without precipitation data and add a variable year. Next, combine precipitation datasets and convert month to a character variable (the variable month.name is built into R and should be useful).

##### Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in both resulting datasets, and give examples of key variables. For available data, what was the total precipitation in 2018? What was the median number of sports balls in a dumpster in 2017?

```{r}
#mr_trash_wheel = readxl::read_excel("./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx") 
```

```{r data_q1}
trashwheel_df = 
  read_xlsx(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    range = cell_cols("A:N"))  %>% # only looking at the specific columns
  janitor::clean_names() %>% #cleaning names 
  drop_na(dumpster)  %>% # only looking at the dumster variable 
  mutate(
    sports_balls = round(sports_balls),
    sports_balls = as.integer(sports_balls)
  )

```

Reading in the precipitation data 
```{r}
precip_2018 =
  read_excel(
     "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
     sheet = "2018 Precipitation",
     skip = 1
  ) %>% 
  janitor::clean_names() %>%
  drop_na(month) %>%
  mutate(year = 2018) %>%
  relocate(year)

precip_2017 =
  read_excel(
     "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
     sheet = "2017 Precipitation",
     skip = 1
  ) %>% 
  janitor::clean_names() %>%
  drop_na(month) %>%
  mutate(year = 2017) %>%
  relocate(year)
```

now combine annual precipitation 

```{r}
month_df = 
  tibble(
    month = 1:12,
    month_name = month.name
  )

precip_df = 
  bind_rows(precip_2017, precip_2018)

left_join(precip_df, month_df, by = "month")
```

This data set contains info from the Mr. Trashwheel trash collector in Baltimore Maryland. As trash enters the inner harbor, the trashwheel collects that trash and stores it in a dumster. The dataset contains info on year month and trash collected including some specific kinds of trash. There are a total of `r nrow(trashwheel_df)` rows in our final dataset. Addition data sheets include monthly precipitation data. 

* The median number of sports balls found in a dumpster in 2017 was `r trashwheel_df %>% filter(year == 2017) %>% pull(sports_balls) %>% median()`
* The total precipitation in 2018 was `r precip_df %>% filter(year == 2018) %>% pull(total) %>% sum()` inches.



# Problem 2
##### This problem focuses on NYC Transit data; in particular, this CSV file contains information related to each entrance and exit for each subway station in NYC.
```{r data_q2, warning=FALSE}
transit_df = read_csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", col_types = cols(
                      .default = col_character(),
                      `Station Latitude` = col_double(),
                      `Station Longitude` = col_double(),
                      #I am forcing these routes to be chars  
                      Route8 = col_character(), 
                      Route9 = col_character(),
                      Route10 = col_character(),
                      Route11 = col_character(),
                      ADA = col_logical(),
                      `Free Crossover` = col_logical(),
                      `Entrance Latitude` = col_double(),
                      `Entrance Longitude` = col_double() 
                      )) %>% 
              janitor::clean_names()  %>% 
              mutate(
                entry = ifelse(entry == "YES", TRUE, FALSE ),
                vending = ifelse(vending == "YES", TRUE, FALSE )
              )
```

##### Read and clean the data; retain line, station, name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance. Convert the entry variable from character (YES vs NO) to a logical variable (the ifelse or recode function may be useful).
```{r clean_q2}
names(transit_df)
```


```{r view_q2}
skimr::skim(transit_df)
```

#### Write a short paragraph about this dataset – explain briefly what variables the dataset contains, describe your data cleaning steps so far, and give the dimension (rows x columns) of the resulting dataset. Are these data tidy?

The transit dataset has  1868 rows and 32 columns all describing subway lines of NYC. If has information such as the subway line, the subways that go to the station, the location, the staffing situation, and much more relevant informtation about the subways platforms. My cleaning process has been cleaning up the names of all the variables and then forcing the route# to all be characters. This way everything is consistent and makes sense. I also checked to make sure that the read_csv was correctly identifying all the variable types and it was except for the route numbers greater than 9. I also convirted all the yes no variables (entry and vending) to true and false. 

##### Answer the following questions using these data:

- How many distinct stations are there? Note that stations are identified both by name and by line (e.g. 125th St A/B/C/D; 125st 1; 125st 4/5); the distinct function may be useful here.
```{r}
unique_stations = transit_df %>% select(station_name, route1, route2,  route3,  
                                        route4, route5, route6, route7, route8, 
                                        route9, route10, route11) %>% 
                                 distinct() %>%
                                 count()
```
There are `r unique_stations` unique stations! 

- How many stations are ADA compliant?
```{r}
transit_df %>% 
  select(ada) %>% 
  table()
```
There are 468 records that are ADA compliant meaning that 468 stations are compliant in this dataset. 

- What proportion of station entrances / exits without vending allow entrance?
```{r}
transit_df %>% 
  select(entry, vending) %>% 
  table()
```

There is a total of 183 stations that do not have vending and within those 183 stations only 69 allow entrance. Thus this proportion of the stations that allow entry / stations that have no vending is 69 / 183 = 0.377 = 37%. However, out of all that stations there are 69 that do not have vending but allow entrance so the proprtion out of all the stations is = no vending but allowed entry / total number of stations = 69/ 1868 = 0.0369 = 3.7%



##### Reformat data so that route number and route name are distinct variables. How many distinct stations serve the A train? Of the stations that serve the A train, how many are ADA compliant?
```{r}
# re formating the data 
transit_df_tidy = 
  pivot_longer(
    transit_df, 
    route1:route11,
    names_to = "route", 
    names_prefix = "route",
    values_to = "route_name") %>% 
  drop_na(route_name) #removing the NA values 

#Filtering for A train, looking at only distinct, counting them all up 
distinct_A <- filter(transit_df_tidy, route_name=="A") %>%
  distinct() %>%
  count()
distinct_A
```

There are `r distinct_A` distinct stations that have access to the A line. 

```{r}
#Filtering for A train and for ADA compliance 
distinct_ada = filter(transit_df_tidy, route_name=="A", ada == TRUE) %>%
  count()
distinct_ada
```
There are `r distinct_ada` stations that have the A train that are ADA compliant.


# Problem 3
##### This problem uses the FiveThirtyEight data; these data were gathered to create the interactive graphic on this page. In particular, we’ll use the data in pols-month.csv, unemployment.csv, and snp.csv. Our goal is to merge these into a single data frame using year and month as keys across datasets.

##### First, clean the data in pols-month.csv. Use separate() to break up the variable mon into integer variables year, month, and day; replace month number with month name; create a president variable taking values gop and dem, and remove prez_dem and prez_gop; and remove the day variable.

```{r}
month_df = 
  tibble(
    month = 1:12,
    month_name = month.name
  )

pols_month = read_csv(file = "./data/fivethirtyeight_datasets/pols-month.csv") %>%
                janitor::clean_names() %>%
                separate(mon, sep="-", into = c("year", "month", "day")) %>%
                mutate(
                 year = as.integer(year),
                 month = as.integer(month),
                 day = as.integer(day)
                ) 
             

pols_month = left_join(pols_month, month_df, by = "month") %>% 
              relocate(year, month_name) %>% #i want the month values closer to front
              mutate(
                 prez_dem = ifelse(prez_dem == 1, TRUE, NA), #puting NA so we can easily drop later
                 prez_gop = ifelse(prez_gop == 1, TRUE, NA) 
              ) 
              
pols_month = pivot_longer( # diretion said to combine the dem and gop prez data
                pols_month, 
                c(prez_dem, prez_gop), 
                names_to = "president", 
                names_prefix = "prez_",
                values_to = "pres_data") %>% 
                drop_na(pres_data) %>% #removing the NA values 
                select(-pres_data) %>% #no longer need the pres_data column becuase it is redundent
                select(-c(day, month)) #directions said to remove day column and month is redundent

       
```

This dataset contains `r nrow(pols_month)` entries and with `r ncol(pols_month)` different columns all related to the politicians who are democratic or republican. We have arranged the data in a way so the year and month is easily reable as well as what polictical party the politician belongs too. 


##### Second, clean the data in snp.csv using a similar process to the above. For consistency across datasets, arrange according to year and month, and organize so that year and month are the leading columns.

```{r}
snp_df = read_csv(file = "./data/fivethirtyeight_datasets/snp.csv")  %>%
                janitor::clean_names() %>%
                separate(date, sep="/", into = c("day", "month", "year")) %>%
                mutate(
                 year = as.integer(year),
                 month = as.integer(month),
                 day = as.integer(day)
                ) 
snp_df = left_join(snp_df, month_df, by = "month") %>% 
              relocate(year, month_name) %>% #rearranging the variables 
              select(-c(month)) # the month value is repetitive so removing 
```

This data set snf_df has `r nrow(snp_df)` entries and `r ncol(snp_df)` different variable columns. It has data related to the stock market index (S&P). The close column relates to what the value of the S&P index was at the end of the day. The other information relates to the date. 



##### Third, tidy the unemployment data so that it can be merged with the previous datasets. This process will involve switching from “wide” to “long” format; ensuring that key variables have the same name; and ensuring that key variables take the same values.
```{r}
unemploy = read_csv(file = "./data/fivethirtyeight_datasets/unemployment.csv")  %>%
          janitor::clean_names() 


unemploy = pivot_longer(
  unemploy, 
  jan:dec,
  names_to = "month",
  values_to = "unemployment_rate")

month_word = 
  tibble(
    month = c("jan", "feb", "mar", "apr", "may", "jun",
              "jul", "aug", "sep", "oct", "nov", "dec"),
    month_name = month.name
  )
unemploy = left_join(unemploy, month_word, by = "month") %>% 
              relocate(year, month_name) %>%
              select(-month)
```

This dataset has `r nrow(unemploy)` entries and `r ncol(unemploy)` different variable columns. The original dataset seperated the unemployment rate by the month but this new cleaned up dataframe has info on the year, month, and the for each year and month it gives the unemployment rate. 


##### Join the datasets by merging snp into pols, and merging unemployment into the result.
```{r}
#first merging snp into pols 
joining_df = left_join(snp_df, pols_month, by=c("year","month_name"))
total_df = left_join(joining_df, unemploy, by=c("year","month_name"))
head(total_df)
```

##### Write a short paragraph about these datasets. Explain briefly what each dataset contained, and describe the resulting dataset (e.g. give the dimension, range of years, and names of key variables).

This final dataframe has `r nrow(total_df)` entries and `r ncol(total_df)` column variables. It is a merged table of all the three tables I talked about earlier. The tables were merged using both the year and the month. It give a look at the political people in office and if they were democratic or republican and shows a quick glances at the economy by looking at the stock market (S&P) values of each given month/year as well as the unemployment rate. Looking specificaly at some important variable we first have `year` which has the years `r total_df %>% select(year) %>% min()` up to `r total_df %>% select(year) %>% max()`. Other key variables are `close` which is the (S&P) close at each day, and the unemployment rate which is under the variable `unemployment_rate`. Another important variable is the `president` which states whether the president was a democrat or republican. 



