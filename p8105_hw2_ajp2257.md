p8105 HW2
================
Amy Pitts
9/30/2020

# Problem 1

This problem uses the Mr. Trash Wheel dataset, available as an Excel
file on the course website.

#### Read and clean the Mr. Trash Wheel sheet:

  - specify the sheet in the Excel file and to omit non-data entries
    (rows with notes / figures; columns containing notes) using
    arguments in read\_excel
  - use reasonable variable names
  - omit rows that do not include dumpster-specific data
  - round the number of sports balls to the nearest integer and converts
    the result to an integer variable (using as.integer)

Read and clean precipitation data for 2017 and 2018. For each, omit rows
without precipitation data and add a variable year. Next, combine
precipitation datasets and convert month to a character variable (the
variable month.name is built into R and should be useful).

Write a paragraph about these data; you are encouraged to use inline R.
Be sure to note the number of observations in both resulting datasets,
and give examples of key variables. For available data, what was the
total precipitation in 2018? What was the median number of sports balls
in a dumpster in 2017?

``` r
#mr_trash_wheel = readxl::read_excel("./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx") 
```

``` r
trashwheel_df = 
  read_xlsx(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    range = cell_cols("A:N"))  %>% # only looking at the specific columns
  janitor::clean_names() %>% #cleaning names 
  drop_na(dumpster)  %>% # only looking at the dumster variable 
  mutate(
    sports_balls = round(sports_balls),
    sports_balls = as.integer(sports_balls)
  )
```

Reading in the precipitation data

``` r
precip_2018 =
  read_excel(
     "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
     sheet = "2018 Precipitation",
     skip = 1
  ) %>% 
  janitor::clean_names() %>%
  drop_na(month) %>%
  mutate(year = 2018) %>%
  relocate(year)

precip_2017 =
  read_excel(
     "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
     sheet = "2017 Precipitation",
     skip = 1
  ) %>% 
  janitor::clean_names() %>%
  drop_na(month) %>%
  mutate(year = 2017) %>%
  relocate(year)
```

now combine annual precipitation

``` r
month_df = 
  tibble(
    month = 1:12,
    month_name = month.name
  )

precip_df = 
  bind_rows(precip_2017, precip_2018)

left_join(precip_df, month_df, by = "month")
```

    ## # A tibble: 24 x 4
    ##     year month total month_name
    ##    <dbl> <dbl> <dbl> <chr>     
    ##  1  2017     1  2.34 January   
    ##  2  2017     2  1.46 February  
    ##  3  2017     3  3.57 March     
    ##  4  2017     4  3.99 April     
    ##  5  2017     5  5.64 May       
    ##  6  2017     6  1.4  June      
    ##  7  2017     7  7.09 July      
    ##  8  2017     8  4.44 August    
    ##  9  2017     9  1.95 September 
    ## 10  2017    10  0    October   
    ## # … with 14 more rows

This data set contains info from the Mr. Trashwheel trash collector in
Baltimore Maryland. As trash enters the inner harbor, the trashwheel
collects that trash and stores it in a dumster. The dataset contains
info on year month and trash collected including some specific kinds of
trash. Ther are a total of 344 rows in our final dataset. Addition data
sheets include monthly preciputation data.

# Problem 2

This problem focuses on NYC Transit data; in particular, this CSV file
contains information related to each entrance and exit for each subway
station in NYC.

``` r
transit_df = read_csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", col_types = cols(
                      .default = col_character(),
                      `Station Latitude` = col_double(),
                      `Station Longitude` = col_double(),
                      #I am forcing these routes to be characters even though they wanted to be doubles. 
                      Route8 = col_character(), 
                      Route9 = col_character(),
                      Route10 = col_character(),
                      Route11 = col_character(),
                      ADA = col_logical(),
                      `Free Crossover` = col_logical(),
                      `Entrance Latitude` = col_double(),
                      `Entrance Longitude` = col_double() 
                      )) %>% 
              janitor::clean_names() #cleaning the names right away 
```

Read and clean the data; retain line, station, name, station latitude /
longitude, routes served, entry, vending, entrance type, and ADA
compliance. Convert the entry variable from character (YES vs NO) to a
logical variable (the ifelse or recode function may be useful).

``` r
names(transit_df)
```

    ##  [1] "division"           "line"               "station_name"      
    ##  [4] "station_latitude"   "station_longitude"  "route1"            
    ##  [7] "route2"             "route3"             "route4"            
    ## [10] "route5"             "route6"             "route7"            
    ## [13] "route8"             "route9"             "route10"           
    ## [16] "route11"            "entrance_type"      "entry"             
    ## [19] "exit_only"          "vending"            "staffing"          
    ## [22] "staff_hours"        "ada"                "ada_notes"         
    ## [25] "free_crossover"     "north_south_street" "east_west_street"  
    ## [28] "corner"             "entrance_latitude"  "entrance_longitude"
    ## [31] "station_location"   "entrance_location"

``` r
#converting the entry varable from char to logical var 
transit_df$entry = ifelse(transit_df$entry == "YES", TRUE, FALSE)
#converting the vending varable from char to logical var 
transit_df$vending = ifelse(transit_df$vending == "YES", TRUE, FALSE)
```

``` r
skimr::skim(transit_df)
```

|                                                  |             |
| :----------------------------------------------- | :---------- |
| Name                                             | transit\_df |
| Number of rows                                   | 1868        |
| Number of columns                                | 32          |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |             |
| Column type frequency:                           |             |
| character                                        | 24          |
| logical                                          | 4           |
| numeric                                          | 4           |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |             |
| Group variables                                  | None        |

Data summary

**Variable type: character**

| skim\_variable       | n\_missing | complete\_rate | min | max | empty | n\_unique | whitespace |
| :------------------- | ---------: | -------------: | --: | --: | ----: | --------: | ---------: |
| division             |          0 |           1.00 |   3 |   3 |     0 |         3 |          0 |
| line                 |          0 |           1.00 |   5 |  17 |     0 |        36 |          0 |
| station\_name        |          0 |           1.00 |   4 |  39 |     0 |       356 |          0 |
| route1               |          0 |           1.00 |   1 |   2 |     0 |        24 |          0 |
| route2               |        848 |           0.55 |   1 |   2 |     0 |        20 |          0 |
| route3               |       1374 |           0.26 |   1 |   2 |     0 |        18 |          0 |
| route4               |       1547 |           0.17 |   1 |   1 |     0 |        13 |          0 |
| route5               |       1630 |           0.13 |   1 |   1 |     0 |        12 |          0 |
| route6               |       1741 |           0.07 |   1 |   1 |     0 |         7 |          0 |
| route7               |       1788 |           0.04 |   1 |   2 |     0 |         7 |          0 |
| route8               |       1820 |           0.03 |   1 |   1 |     0 |         3 |          0 |
| route9               |       1840 |           0.01 |   1 |   1 |     0 |         2 |          0 |
| route10              |       1845 |           0.01 |   1 |   1 |     0 |         1 |          0 |
| route11              |       1845 |           0.01 |   1 |   1 |     0 |         1 |          0 |
| entrance\_type       |          0 |           1.00 |   4 |   9 |     0 |         7 |          0 |
| exit\_only           |       1812 |           0.03 |   3 |   3 |     0 |         1 |          0 |
| staffing             |          0 |           1.00 |   4 |   6 |     0 |         4 |          0 |
| staff\_hours         |       1828 |           0.02 |  16 |  33 |     0 |        16 |          0 |
| ada\_notes           |       1793 |           0.04 |   5 |  17 |     0 |        10 |          0 |
| north\_south\_street |         29 |           0.98 |   4 |  23 |     0 |       307 |          0 |
| east\_west\_street   |         35 |           0.98 |   6 |  24 |     0 |       352 |          0 |
| corner               |         32 |           0.98 |   1 |   4 |     0 |         8 |          0 |
| station\_location    |          0 |           1.00 |  20 |  23 |     0 |       472 |          0 |
| entrance\_location   |          0 |           1.00 |  22 |  23 |     0 |      1857 |          0 |

**Variable type: logical**

| skim\_variable  | n\_missing | complete\_rate | mean | count               |
| :-------------- | ---------: | -------------: | ---: | :------------------ |
| entry           |          0 |              1 | 0.94 | TRU: 1753, FAL: 115 |
| vending         |          0 |              1 | 0.90 | TRU: 1685, FAL: 183 |
| ada             |          0 |              1 | 0.25 | FAL: 1400, TRU: 468 |
| free\_crossover |          0 |              1 | 0.78 | TRU: 1448, FAL: 420 |

**Variable type: numeric**

| skim\_variable      | n\_missing | complete\_rate |    mean |   sd |      p0 |     p25 |     p50 |     p75 |    p100 | hist  |
| :------------------ | ---------: | -------------: | ------: | ---: | ------: | ------: | ------: | ------: | ------: | :---- |
| station\_latitude   |          0 |              1 |   40.73 | 0.07 |   40.58 |   40.69 |   40.73 |   40.77 |   40.90 | ▂▅▇▃▂ |
| station\_longitude  |          0 |              1 | \-73.94 | 0.06 | \-74.03 | \-73.99 | \-73.96 | \-73.91 | \-73.76 | ▇▆▃▂▁ |
| entrance\_latitude  |          0 |              1 |   40.73 | 0.07 |   40.58 |   40.69 |   40.73 |   40.77 |   40.90 | ▂▅▇▃▂ |
| entrance\_longitude |          0 |              1 | \-73.86 | 3.42 | \-74.03 | \-73.99 | \-73.96 | \-73.91 |   73.99 | ▇▁▁▁▁ |

#### Write a short paragraph about this dataset – explain briefly what variables the dataset contains, describe your data cleaning steps so far, and give the dimension (rows x columns) of the resulting dataset. Are these data tidy?

The transit dataset has 1868 rows and 32 columns all describing subway
lines of NYC. If has information such as the subway line, the subways
that go to the station, the location, the staffing situation, and much
more relevant informtation about the subways platforms. My cleaning
process has been cleaning up the names of all the variables and then
forcing the route\# to all be characters. This way everything is
consistent and makes sense. I also checked to make sure that the
read\_csv was correctly identifying all the variable types and it was
except for the route numbers greater than 9. I also convirted all the
yes no variables (entry and vending) to true and false.

#### Answer the following questions using these data:

  - How many distinct stations are there? Note that stations are
    identified both by name and by line (e.g. 125th St A/B/C/D; 125st 1;
    125st 4/5); the distinct function may be useful here.

<!-- end list -->

``` r
small_transit_df = data.frame(transit_df$station_name, 
                              transit_df$route1, 
                              transit_df$route2, 
                              transit_df$route3, 
                              transit_df$route4, 
                              transit_df$route5, 
                              transit_df$route6, 
                              transit_df$route7, 
                              transit_df$route8, 
                              transit_df$route9, 
                              transit_df$route10, 
                              transit_df$route11)
count(distinct(small_transit_df))
```

    ##     n
    ## 1 456

There are 456 unique stations\!

  - How many stations are ADA compliant?

<!-- end list -->

``` r
table(transit_df$ada)
```

    ## 
    ## FALSE  TRUE 
    ##  1400   468

There are 468 records that are ADA compliant meaning that 468 stations
are compliant in this dataset.

  - What proportion of station entrances / exits without vending allow
    entrance?

<!-- end list -->

``` r
t = tableby(entry ~ vending , data=transit_df)
summary(t, text=TRUE, title = "Entry by Vending ")
```

    ## 
    ## Table: Entry by Vending 
    ## 
    ## |         | FALSE (N=115) | TRUE (N=1753) | Total (N=1868) | p value|
    ## |:--------|:-------------:|:-------------:|:--------------:|-------:|
    ## |vending  |               |               |                | < 0.001|
    ## |-  FALSE |  114 (99.1%)  |   69 (3.9%)   |   183 (9.8%)   |        |
    ## |-  TRUE  |   1 (0.9%)    | 1684 (96.1%)  |  1685 (90.2%)  |        |

There is a total of 183 stations that do not have vending and within
those 183 stations only 69 allow entrance. Thus this proportion of the
stations that allow entry / stations that have no vending is 69 / 183 =
0.377 = 37%. However, out of all that stations there are 69 that do not
have vending but allow entrance so the proprtion out of all the stations
is = no vending but allowed entry / total number of stations = 69/ 1868
= 0.0369 = 3.7%

#### Reformat data so that route number and route name are distinct variables. How many distinct stations serve the A train? Of the stations that serve the A train, how many are ADA compliant?

``` r
transit_df_tidy = 
  pivot_longer(
    transit_df, 
    route1:route11,
    names_to = "route", 
    names_prefix = "route",
    values_to = "route_name") %>% 
  drop_na(route_name) #removing the NA values 
head(transit_df_tidy) 
```

    ## # A tibble: 6 x 23
    ##   division line  station_name station_latitude station_longitu… entrance_type
    ##   <chr>    <chr> <chr>                   <dbl>            <dbl> <chr>        
    ## 1 BMT      4 Av… 25th St                  40.7            -74.0 Stair        
    ## 2 BMT      4 Av… 25th St                  40.7            -74.0 Stair        
    ## 3 BMT      4 Av… 36th St                  40.7            -74.0 Stair        
    ## 4 BMT      4 Av… 36th St                  40.7            -74.0 Stair        
    ## 5 BMT      4 Av… 36th St                  40.7            -74.0 Stair        
    ## 6 BMT      4 Av… 36th St                  40.7            -74.0 Stair        
    ## # … with 17 more variables: entry <lgl>, exit_only <chr>, vending <lgl>,
    ## #   staffing <chr>, staff_hours <chr>, ada <lgl>, ada_notes <chr>,
    ## #   free_crossover <lgl>, north_south_street <chr>, east_west_street <chr>,
    ## #   corner <chr>, entrance_latitude <dbl>, entrance_longitude <dbl>,
    ## #   station_location <chr>, entrance_location <chr>, route <chr>,
    ## #   route_name <chr>

``` r
#Filtering for A train, looking at only distinct, counting them all up 
filter(transit_df_tidy, route_name=="A") %>%
  distinct() %>%
  count()
```

    ## # A tibble: 1 x 1
    ##       n
    ##   <int>
    ## 1   273

There are 273 distinct stations that have access to the A line.

``` r
#Filtering for A train and for ADA compliance 
filter(transit_df_tidy, route_name=="A", ada == TRUE) %>%
  count()
```

    ## # A tibble: 1 x 1
    ##       n
    ##   <int>
    ## 1   107

There are 107 stations that have the A train that are ADA compliant.

# Problem 3

This problem uses the FiveThirtyEight data; these data were gathered to
create the interactive graphic on this page. In particular, we’ll use
the data in pols-month.csv, unemployment.csv, and snp.csv. Our goal is
to merge these into a single data frame using year and month as keys
across datasets.

First, clean the data in pols-month.csv. Use separate() to break up the
variable mon into integer variables year, month, and day; replace month
number with month name; create a president variable taking values gop
and dem, and remove prez\_dem and prez\_gop; and remove the day
variable.

Second, clean the data in snp.csv using a similar process to the above.
For consistency across datasets, arrange according to year and month,
and organize so that year and month are the leading columns.

Third, tidy the unemployment data so that it can be merged with the
previous datasets. This process will involve switching from “wide” to
“long” format; ensuring that key variables have the same name; and
ensuring that key variables take the same values.

Join the datasets by merging snp into pols, and merging unemployment
into the result.

Write a short paragraph about these datasets. Explain briefly what each
dataset contained, and describe the resulting dataset (e.g. give the
dimension, range of years, and names of key variables).

Note: we could have used a date variable as a key instead of creating
year and month keys; doing so would help with some kinds of plotting,
and be a more accurate representation of the data. Date formats are
tricky, though. For more information check out the lubridate package in
the tidyverse
